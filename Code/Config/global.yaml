llm:
  active_provider: openrouter
  active_model: meta-llama/llama-3.3-70b-instruct:free
  temperature: 0.7
  max_tokens: 4096
  timeout: 120
  
  # Audit specific
  audit_provider: openrouter
  audit_model: meta-llama/llama-3.3-70b-instruct:free
  audit_temperature: 0.1
  
  # Worker specific
  worker_provider: openrouter
  worker_model: meta-llama/llama-3.3-70b-instruct:free
  worker_temperature: 0.5

  providers:
    openrouter:
      enabled: true
      default_model: meta-llama/llama-3.3-70b-instruct:free
      models:
        - meta-llama/llama-3.3-70b-instruct:free
        - google/gemini-2.0-flash-exp:free
    openai:
      enabled: true
      default_model: gpt-4-turbo
    ollama:
      enabled: true
      host: http://localhost:11434
    vllm:
      enabled: false

  resilience:
    rate_limit: 5.0
    retry_enabled: true
    max_retries: 3
    retry_base_delay: 1.0
    fallback_enabled: true
