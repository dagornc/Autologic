# Architecture du Projet AutoLogic

## Vue d'Ensemble

AutoLogic est un systÃ¨me d'agent IA implÃ©mentant le **Self-Discovery Reasoning Framework**. L'architecture est divisÃ©e en deux parties principales : un Backend Python (FastAPI) et un Frontend React avec une interface glassmorphism moderne.

---

## Arborescence du Projet

```
AutoLogic/
â”œâ”€â”€ Cmd/                        # Scripts shell standalone
â”‚   â”œâ”€â”€ start_backend.sh
â”‚   â””â”€â”€ start_frontend.sh
â”œâ”€â”€ Code/
â”‚   â”œâ”€â”€ Backend/
â”‚   â”‚   â”œâ”€â”€ Phase1-Ingestion/   # [Futur] Pipeline d'ingestion RAG
â”‚   â”‚   â””â”€â”€ Phase2-Inference/   # Logique de raisonnement
â”‚   â”‚       â””â”€â”€ 01_Reasoning/
â”‚   â”‚           â””â”€â”€ autologic/  # Package principal
â”‚   â”‚               â”œâ”€â”€ core/           # Moteur, LLM, modÃ¨les, rÃ©silience
â”‚   â”‚               â”œâ”€â”€ routers/        # Endpoints FastAPI
â”‚   â”‚               â”œâ”€â”€ data/           # DonnÃ©es statiques (modules)
â”‚   â”‚               â””â”€â”€ utils/          # Logging, helpers
â”‚   â””â”€â”€ Frontend/               # Application React/Vite
â”‚       â””â”€â”€ src/
â”‚           â”œâ”€â”€ components/     # Composants UI
â”‚           â”‚   â”œâ”€â”€ ui/             # Composants atomiques (Button, Input...)
â”‚           â”‚   â”œâ”€â”€ AutoLogicInterface.tsx
â”‚           â”‚   â”œâ”€â”€ SettingsDrawer.tsx
â”‚           â”‚   â”œâ”€â”€ Sidebar.tsx
â”‚           â”‚   â””â”€â”€ ThemeProvider.tsx
â”‚           â”œâ”€â”€ contexts/       # Contextes React (Theme)
â”‚           â”œâ”€â”€ hooks/          # Custom hooks (useAutoLogic)
â”‚           â”œâ”€â”€ services/       # Appels API (apiClient)
â”‚           â””â”€â”€ types/          # Types TypeScript
â”œâ”€â”€ Config/
â”‚   â””â”€â”€ global.yaml             # Configuration centralisÃ©e
â”œâ”€â”€ Doc/
â”‚   â”œâ”€â”€ sphinx/                 # Documentation gÃ©nÃ©rÃ©e
â”‚   â”œâ”€â”€ ARCHITECTURE.md         # Ce fichier
â”‚   â””â”€â”€ SETUP.md                # Guide d'installation
â”œâ”€â”€ Log/                        # Fichiers de logs
â”œâ”€â”€ Test/                       # Tests automatisÃ©s (pytest)
â”œâ”€â”€ .env                        # Variables d'environnement
â”œâ”€â”€ requirements.txt            # DÃ©pendances Python
â””â”€â”€ start.sh                    # Script de lancement
```

---

## Architecture Backend

### Vue d'Ensemble des Modules Core

```mermaid
classDiagram
    class BaseLLM {
        <<abstract>>
        +call(prompt: str) str
        +model_name: str
        +provider_name: str
    }
    
    class OpenRouterLLM {
        -api_key: str
        -model_name: str
        -resilient_caller: ResilientCaller
        +call(prompt: str) str
        +list_models() List~str~
        +list_models_detailed() List~dict~
    }
    
    class OpenAILLM {
        -api_key: str
        -model_name: str
        +call(prompt: str) str
        +list_models() List~str~
    }
    
    class OllamaLLM {
        -host: str
        -model_name: str
        +call(prompt: str) str
        +list_models() List~str~
    }
    
    class VLlmLLM {
        -host: str
        +call(prompt: str) str
        +list_models() List~str~
    }
    
    class HuggingFaceLLM {
        -api_key: str
        +call(prompt: str) str
        +list_models() List~str~
    }
    
    class AutoLogicEngine {
        -root_model: BaseLLM
        -worker_model: BaseLLM
        -reasoning_modules: List~ReasoningModule~
        +select_modules(task: str) List
        +adapt_modules(modules, task) List
        +structure_reasoning(adapted, task) ReasoningPlan
        +execute_with_plan(task, plan) Result
        +run_full_cycle(task: str) Result
    }
    
    class ResilientCaller {
        -config: ResilienceConfig
        -rate_limiter: RateLimiter
        +call(func, *args, fallback_func) Any
    }
    
    class RateLimiter {
        -rate: float
        -tokens: float
        +acquire() void
        +set_rate(rate: float) void
    }
    
    BaseLLM <|-- OpenRouterLLM
    BaseLLM <|-- OpenAILLM
    BaseLLM <|-- OllamaLLM
    BaseLLM <|-- VLlmLLM
    BaseLLM <|-- HuggingFaceLLM
    OpenRouterLLM --> ResilientCaller
    ResilientCaller --> RateLimiter
    AutoLogicEngine --> BaseLLM
```

### Modules du Package autologic/core/

| Module | Description |
|--------|-------------|
| `engine.py` | Moteur principal AutoLogicEngine avec cycle Self-Discovery |
| `llm_provider.py` | ImplÃ©mentations des 5 providers LLM (OpenRouter, OpenAI, Ollama, vLLM, HuggingFace) |
| `provider_factory.py` | Factory pattern pour crÃ©ation dynamique de LLM |
| `model_registry.py` | Registre centralisÃ© des modÃ¨les et configurations |
| `resilience.py` | Rate limiter, retry avec backoff, fallback intelligent |
| `models.py` | ModÃ¨les Pydantic (ReasoningModule, AdaptedModule, ReasoningPlan) |
| `prompts.py` | Templates de prompts pour chaque phase |

### Les 4 Phases du Cycle Self-Discovery

```mermaid
flowchart TB
    subgraph Phase1["ðŸ” PHASE 1: SELECT"]
        A[TÃ¢che utilisateur] --> B[Analyse sÃ©mantique]
        B --> C[SÃ©lection parmi 39 modules]
        C --> D[Modules pertinents]
    end
    
    subgraph Phase2["ðŸ”§ PHASE 2: ADAPT"]
        D --> E[Contextualisation]
        E --> F[Modules adaptÃ©s au problÃ¨me]
    end
    
    subgraph Phase3["ðŸ“ PHASE 3: STRUCTURE"]
        F --> G[Ordonnancement logique]
        G --> H[Plan de raisonnement]
    end
    
    subgraph Phase4["âš¡ PHASE 4: EXECUTE"]
        H --> I[ExÃ©cution pas-Ã -pas]
        I --> J[SynthÃ¨se finale]
        J --> K[Solution]
    end
    
    style Phase1 fill:#1a1b26,stroke:#7aa2f7
    style Phase2 fill:#1a1b26,stroke:#9ece6a
    style Phase3 fill:#1a1b26,stroke:#bb9af7
    style Phase4 fill:#1a1b26,stroke:#f7768e
```

| Phase | ModÃ¨le | Description |
|-------|--------|-------------|
| **SELECT** | Root LLM | Analyse la tÃ¢che et sÃ©lectionne les modules pertinents |
| **ADAPT** | Root LLM | Transforme les modules gÃ©nÃ©riques en instructions spÃ©cifiques |
| **STRUCTURE** | Root LLM | Ordonne les modules en un plan de raisonnement cohÃ©rent |
| **EXECUTE** | Worker LLM | Suit le plan pour gÃ©nÃ©rer la solution finale |

### Architecture de RÃ©silience

```mermaid
flowchart LR
    subgraph Request["RequÃªte LLM"]
        A[Appel API]
    end
    
    subgraph Resilience["Couche RÃ©silience"]
        B[RateLimiter]
        C[Retry Logic]
        D[Fallback Handler]
    end
    
    subgraph Response["RÃ©ponse"]
        E[SuccÃ¨s]
        F[Fallback Model]
        G[Erreur]
    end
    
    A --> B
    B --> C
    C --> |"429/5xx"| C
    C --> |"Max retries"| D
    C --> |"Success"| E
    D --> |"Fallback enabled"| F
    D --> |"Fallback disabled"| G
    
    style Resilience fill:#1a1b26,stroke:#f7768e
```

| Composant | Fonction |
|-----------|----------|
| **RateLimiter** | Token bucket (dÃ©faut: 5 req/s) |
| **Retry** | Backoff exponentiel sur 429/5xx (max 3 retries) |
| **Fallback** | Bascule vers modÃ¨le alternatif gratuit |

### Routers FastAPI

```mermaid
graph LR
    Client[Client HTTP] --> API[FastAPI App]
    
    API --> R1["/reason/*"]
    API --> R2["/api/*"]
    API --> R3["/, /health"]
    
    R1 --> E1[POST /reason/full]
    R1 --> E2[GET /reason/modules]
    
    R2 --> E3[GET /api/models]
    R2 --> E4[GET /api/providers/config]
    R2 --> E5[PUT /api/providers/config]
    R2 --> E6[GET /api/providers/status]
    R2 --> E7["GET /api/providers/{p}/models"]
    R2 --> E8[POST /api/providers/verify]
    R2 --> E9["GET/PUT /api/providers/{p}/resilience"]
    
    E1 --> Engine[AutoLogicEngine]
    E2 --> Engine
    E3 --> Registry[ModelRegistry]
    E4 --> Registry
```

### Liste des Endpoints

| Route | MÃ©thode | Handler | Description |
|-------|---------|---------|-------------|
| `/` | GET | `root()` | Health check basique |
| `/health` | GET | `health_check()` | Status dÃ©taillÃ© |
| `/reason/full` | POST | `solve_task()` | Cycle complet Self-Discover |
| `/reason/modules` | GET | `list_modules()` | Liste des 39 modules |
| `/api/models` | GET | `list_models()` | Providers et modÃ¨les LLM |
| `/api/providers/config` | GET | `get_providers_config()` | Config active |
| `/api/providers/config` | PUT | `update_providers_config()` | Mise Ã  jour config |
| `/api/providers/status` | GET | `get_providers_status()` | Status des providers |
| `/api/providers/{provider}/models` | GET | `get_provider_models()` | ModÃ¨les d'un provider |
| `/api/providers/verify` | POST | `verify_provider_connection()` | Test de connexion |
| `/api/providers/{provider}/resilience` | GET/PUT | Config rÃ©silience | ParamÃ¨tres de rÃ©silience |

---

## Architecture Frontend

### Composants Principaux

```mermaid
graph TB
    App[App.tsx] --> ALI[AutoLogicInterface]
    ALI --> TP[ThemeProvider]
    TP --> Content[AutoLogicContent]
    
    Content --> Sidebar
    Content --> Header
    Content --> SettingsDrawer
    Content --> TaskInput
    Content --> LoadingOverlay
    Content --> ErrorMessage
    Content --> Results[Results Section]
    
    Results --> PlanDisplay
    Results --> SolutionDisplay
    
    SettingsDrawer --> SD1[Provider Select]
    SettingsDrawer --> SD2[Model Search]
    SettingsDrawer --> SD3[API Key Input]
    SettingsDrawer --> SD4[Parameters Sliders]
    SettingsDrawer --> SD5[Resilience Config]
    SettingsDrawer --> SD6[Connection Test]
    
    Content --> Hook[useAutoLogic Hook]
    Hook --> API[apiClient Service]
```

### Composants UI

| Composant | Fichier | Description |
|-----------|---------|-------------|
| **AutoLogicInterface** | `AutoLogicInterface.tsx` | Layout principal avec sidebar et contenu |
| **SettingsDrawer** | `SettingsDrawer.tsx` | Panneau de configuration avancÃ©e |
| **Sidebar** | `Sidebar.tsx` | Navigation latÃ©rale |
| **ThemeProvider** | `ThemeProvider.tsx` | Gestion du thÃ¨me (dark/light) |
| **UI Atoms** | `ui/*.tsx` | Button, Input, Slider, etc. |

### Structure des Types

```typescript
// Types principaux
interface ReasoningPlan {
  steps: ReasoningPlanStep[];
  estimated_complexity: 'low' | 'medium' | 'high';
  total_steps: number;
}

interface AutoLogicResult {
  task: string;
  plan: ReasoningPlan;
  final_output: string;
}

interface LLMConfig {
  provider: string;
  model: string;
  temperature: number;
  maxTokens: number;
  topP: number;
}

interface ResilienceConfig {
  rateLimit: number;
  retryEnabled: boolean;
  maxRetries: number;
  fallbackEnabled: boolean;
}
```

### Service API Client

Le `apiClient` centralise tous les appels backend :

```typescript
// services/api.ts
const apiClient = {
  // Raisonnement
  solveTask(task: string, config?: LLMConfig): Promise<AutoLogicResult>,
  getModules(): Promise<ReasoningModule[]>,
  
  // Configuration
  getProvidersConfig(): Promise<ProviderConfig>,
  updateProvidersConfig(config: ProviderConfig): Promise<void>,
  
  // Providers
  getProvidersStatus(): Promise<ProviderStatus[]>,
  getProviderModels(provider: string, apiKey?: string): Promise<string[]>,
  verifyConnection(provider: string, apiKey?: string): Promise<boolean>,
  
  // RÃ©silience
  getResilienceConfig(provider: string): Promise<ResilienceConfig>,
  updateResilienceConfig(provider: string, config: ResilienceConfig): Promise<void>,
};
```

---

## Flux de DonnÃ©es Complet

```mermaid
sequenceDiagram
    participant U as Utilisateur
    participant F as Frontend
    participant B as Backend API
    participant E as AutoLogicEngine
    participant R as ResilientCaller
    participant L as LLM Provider
    
    U->>F: Saisit une tÃ¢che
    F->>B: POST /reason/full {task, config}
    B->>E: run_full_cycle(task)
    
    Note over E: Phase 1: SELECT
    E->>R: call(select_prompt)
    R->>R: Rate Limit Check
    R->>L: API Call
    L-->>R: Response / Error
    R->>R: Retry/Fallback if needed
    R-->>E: Modules sÃ©lectionnÃ©s
    
    Note over E: Phase 2: ADAPT
    E->>R: call(adapt_prompt)
    R->>L: API Call
    L-->>R: Response
    R-->>E: Modules adaptÃ©s
    
    Note over E: Phase 3: STRUCTURE
    E->>R: call(structure_prompt)
    R->>L: API Call
    L-->>R: Response
    R-->>E: Plan de raisonnement
    
    Note over E: Phase 4: EXECUTE
    E->>R: call(execute_prompt)
    R->>L: API Call
    L-->>R: Response
    R-->>E: Solution finale
    
    E-->>B: {task, plan, final_output}
    B-->>F: JSON Response
    F-->>U: Affiche plan + solution
```

---

## Configuration

### Variables d'Environnement

| Variable | Description | DÃ©faut |
|----------|-------------|--------|
| `OPENROUTER_API_KEY` | ClÃ© API OpenRouter | - |
| `OPENAI_API_KEY` | ClÃ© API OpenAI | - |
| `OLLAMA_HOST` | URL serveur Ollama | `http://localhost:11434` |
| `HUGGINGFACE_API_KEY` | ClÃ© API HuggingFace | - |
| `LOG_LEVEL` | Niveau de log | `INFO` |
| `CORS_ORIGINS` | Origines CORS autorisÃ©es | `http://localhost:5173` |

### global.yaml

```yaml
app:
  name: "AutoLogic"
  version: "0.2.0"
  environment: "development"

llm:
  active_provider: "openrouter"
  active_model: "google/gemini-2.0-flash-exp:free"
  temperature: 0.7
  max_tokens: 4096
  timeout: 180
  
  resilience:
    rate_limit: 5.0
    retry_enabled: true
    max_retries: 3
    retry_base_delay: 2.0
    fallback_enabled: true
  
  providers:
    openrouter:
      enabled: true
      base_url: "https://openrouter.ai/api/v1"
    openai:
      enabled: true
    ollama:
      enabled: true
      base_url: "http://localhost:11434"
    vllm:
      enabled: false
    huggingface:
      enabled: true

vector_store:
  provider: "chromadb"
  path: "./data/chroma"

logging:
  level: "INFO"
  file: "Log/backend_app.log"
```

---

## Bonnes Pratiques

### Backend
- **Typage strict** : Tous les modÃ¨les utilisent Pydantic avec validation
- **Async** : Endpoints asynchrones pour performance
- **Logging structurÃ©** : Logs dans `Log/backend_app.log`
- **Injection de dÃ©pendances** : Via FastAPI `Depends()`
- **Factory Pattern** : Pour crÃ©ation dynamique de LLM
- **RÃ©silience** : Rate limiting, retry, fallback intÃ©grÃ©s

### Frontend
- **Composants atomiques** : UI modulaire et rÃ©utilisable
- **Custom hooks** : Logique mÃ©tier isolÃ©e (`useAutoLogic`)
- **Types TypeScript** : Typage strict partagÃ© avec le backend
- **Animations** : Framer Motion pour UX fluide
- **Design Glassmorphism** : Effets visuels modernes

### SÃ©curitÃ©
- **ClÃ©s API** : Stockage sÃ©curisÃ© via localStorage chiffrÃ© (frontend) et .env (backend)
- **CORS** : Configuration stricte des origines autorisÃ©es
- **Validation** : Pydantic pour toutes les entrÃ©es API

---

## Diagramme de DÃ©ploiement

```mermaid
graph TB
    subgraph "Client"
        Browser[Navigateur]
    end
    
    subgraph "Frontend (Vite)"
        React[React App<br>:5173]
    end
    
    subgraph "Backend (FastAPI)"
        API[FastAPI<br>:8000]
        Engine[AutoLogic<br>Engine]
        Registry[Model<br>Registry]
    end
    
    subgraph "LLM Providers"
        OR[OpenRouter]
        OA[OpenAI]
        OL[Ollama<br>:11434]
        VL[vLLM]
        HF[HuggingFace]
    end
    
    Browser --> React
    React --> API
    API --> Engine
    API --> Registry
    Engine --> OR
    Engine --> OA
    Engine --> OL
    Engine --> VL
    Engine --> HF
    
    style Browser fill:#1a1b26,stroke:#7aa2f7
    style React fill:#1a1b26,stroke:#bb9af7
    style API fill:#1a1b26,stroke:#9ece6a
```

---

*Version 0.2.0 - Janvier 2025*
